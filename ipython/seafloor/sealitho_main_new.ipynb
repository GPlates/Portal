{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sea Floor Lithology Feature Evaluator\n",
    "\n",
    "This feature evaluator uses probabilistic Gaussian process classification to estimate the significance \n",
    "of features in predicting the sea floor sediment.\n",
    "\n",
    "Five real features and one random feature are used in the evaluation. \n",
    "    \n",
    "    Feature ID      Feature Name   \n",
    "        1           bathymetry\n",
    "        2           silicate\n",
    "        3           productivity\n",
    "        4           salinity\n",
    "        5           temperature\n",
    "        6           random\n",
    "        \n",
    "The types of sea floor sediment have been grouped into five catagories.\n",
    "\n",
    "    Sediment                                             Label\n",
    "    gravel, sand, silt                                     1\n",
    "    clay                                                   2\n",
    "    calcareous ooze, fine grained calcareous sediment      3 \n",
    "    radiolarian ooze                                       4\n",
    "    diatom ooze, sponge spicules                           5\n",
    "\n",
    "Since this is a probabilistic process, the number of iterations affects the accuracy of the evaluation. \n",
    "In order to get a relatively reasonable evaluation, at least 30 iterations are required. However, it takes long time\n",
    "to run 30 iterations. So, if you just want to get a sense of how the evaluator works, you may try 5 iterations first.\n",
    "And then, if interested, you can add more iterations later.\n",
    "\n",
    " - Create the evaluator\n",
    " \n",
    "    The constructor of evaluator takes the label of sediment as input parameter.\n",
    "    The list of labels are given above. Each evaluator can only handle one label.\n",
    "    \n",
    "    For example:\n",
    "        import sealitho_lib as sfl\n",
    "        e = sfl.FeatureEvaluator(5)\n",
    "        \n",
    " - Start the evaluation process (WARNING:THE FUNCTION MAY TAKE LONG TIME TO RETURN.)\n",
    " \n",
    "     The doFeatureEvaluation() function takes the number of iterations as input parameter.\n",
    "     Each time the function is called, the evalution result will be appended to previous results.\n",
    "     \n",
    "     For example:\n",
    "        e.doFeatureEvaluation(5)\n",
    "        e.doFeatureEvaluation(5)\n",
    "     The above code equals to e.doFeatureEvaluation(10).\n",
    "     \n",
    " - Get the evaluation result\n",
    " \n",
    "     The result is in a three dimension array.\n",
    "     The first dimension is the iteration.\n",
    "     The second dimension is the feature combination.\n",
    "     The third dimension is a tuple which contains two items. The first one is the feature combination in a tuple.The second one is the score.\n",
    "     \n",
    "     For example, \n",
    "         [\n",
    "             [\n",
    "                 ((1,2), 0.80497592295345111),\n",
    "                 ((1,2,3), 0.80497592295345111)\n",
    "             ],\n",
    "             [\n",
    "                 ((3), 0.80497592295345111),\n",
    "                 ((2,3), 0.80497592295345111)\n",
    "             ]\n",
    "         ]\n",
    "         \n",
    "     The example above has two iterations and each iteration has two feature combinations. \n",
    "     In real world, each iteration has 62 feature combinations which is the number of \n",
    "     all possible combinations of 6 features except the empty and full set(math.pow(2, 6)-2=62). \n",
    "     Basically, it means all numbers between 0b000001 and 0b111110.\n",
    "     \n",
    "     Code example:\n",
    "         e.getResults()\n",
    "    \n",
    " -  getMeanScores()\n",
    " \n",
    "     Get the mean scores and standard deviation for each feature combination.\n",
    "     \n",
    "     For example:\n",
    "         [\n",
    "             ((1, 2, 4, 5), 0.80709291475581169, 0.022290250761077178),\n",
    "             ((2, 3, 5), 0.75709283934338834, 0.023438694405096404),\n",
    "                 ......\n",
    "             ((1, 3), 0.7968714536826742, 0.043176718930593445)\n",
    "         ]\n",
    "         \n",
    "     Code example:\n",
    "         e.getMeanScores()\n",
    "        \n",
    "    \n",
    " -  getHighestMeanScores()\n",
    " \n",
    "     Get the highest mean scores for each length of feature combinations.\n",
    "     \n",
    "     For example: the highest mean scores for feature combination length 1,2,3,4,5.\n",
    "        [\n",
    "             ((1,), 0.79627967403009137, 0.030071120117350029),\n",
    "             ((3, 4), 0.80339827798743924, 0.035320722651528164),\n",
    "             ((1, 2, 4), 0.83070040655888044, 0.012610268538728072),\n",
    "             ((1, 2, 4, 5), 0.83746814502501521, 0.025451271286306136),\n",
    "             ((1, 2, 3, 4, 5), 0.82545392324219369, 0.016482177962087206)\n",
    "        ]\n",
    "        \n",
    "    Code example:\n",
    "        e.getHighestMeanScores()\n",
    "            \n",
    " - getHighestScores()\n",
    " \n",
    "    Get the highest scores for each iteration.\n",
    "     \n",
    "     For example: the highest scores for two iterations\n",
    "         [\n",
    "             [\n",
    "                 ((3,), 0.76658640984483684),\n",
    "                 ((1, 3), 0.81270053475935822),\n",
    "                 ((1, 4, 5), 0.85954301075268824),\n",
    "                 ((1, 2, 4, 5), 0.85808094344679708),\n",
    "                 ((1, 2, 3, 4, 5), 0.82132253711201075)\n",
    "             ],\n",
    "             [\n",
    "                 ((1,), 0.79973262032085568),\n",
    "                 ((2, 3), 0.82170542635658905),\n",
    "                 ((1, 2, 3), 0.83983957219251337),\n",
    "                 ((1, 2, 3, 5), 0.87148268398268391),\n",
    "                 ((1, 2, 4, 5, 6), 0.84572192513368982)\n",
    "             ]\n",
    "         ]\n",
    "     \n",
    " - printFeatureEvalPlot()\n",
    " \n",
    "    Plot the highest mean scores and its standard deviation.\n",
    "     \n",
    "     Code example:\n",
    "         e.printFeatureEvalPlot()\n",
    "     \n",
    " - printFeatureEvalMatrix()\n",
    " \n",
    "    Print a matrix which indicates how often a feature shows up in the highest score feature combinations.\n",
    "    For example, the color at location (1,5) means how often feature 1(bathymetry) is in the feature combinations with  highest score when 5 features are used. In general, the warmer the color is, the more significant the feature is.\n",
    "     \n",
    "     Code example:\n",
    "         e.printFeatureEvalMatrix()\n",
    "         \n",
    "         \n",
    "# For more information, please read the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sealitho_lib as sfl\n",
    "\n",
    "#Labels\n",
    "gravel_sand_silt = 1\n",
    "clay = 2\n",
    "calcareous_ooze_fine_grained_sediment = 3\n",
    "radiolarian_ooze = 4\n",
    "diatom_ooze_sponge_spicules = 5\n",
    "\n",
    "iterationNumber = 5\n",
    "\n",
    "e = sfl.FeatureEvaluator(diatom_ooze_sponge_spicules)\n",
    "#run some iterations and will run more later so that we can see the differences in results.\n",
    "#WARNING: THIS WILL TAKE A WHILE\n",
    "e.doFeatureEvaluation(iterationNumber)\n",
    "e.printFeatureEvalPlot()\n",
    "e.printFeatureEvalMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run more iterations and print plot and matrix\n",
    "\n",
    "#WARNING: THIS WILL TAKE A WHILE. \n",
    "#IF YOU DO NOT WANT MORE ITERATIONS, YOU CAN SKIP THIS CELL.\n",
    "#Run 30 iteration may take several hours depending on how many CPUs the computer has.\n",
    "\n",
    "iterationNumber=30\n",
    "\n",
    "e.doFeatureEvaluation(iterationNumber)\n",
    "e.printFeatureEvalPlot()\n",
    "e.printFeatureEvalMatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#retrieve evaluation data\n",
    "\n",
    "import numpy as np\n",
    "import sealitho_lib as sfl\n",
    "\n",
    "for f in e.getHighestMeanScores():\n",
    "    if(len(f[0])==1):\n",
    "        print('The most informative feature is (\\'{0}\\') ({1}) when the model is trained with a single feature.'\\\n",
    "              .format(sfl.featureToString(f[0])[0], f[1]))\n",
    "    else:\n",
    "        print('The most informative features are {0} ({2})when the model is trained with {1} features.'\\\n",
    "              .format(sfl.featureToString(f[0]), len(f[0]), f[1]))\n",
    "\n",
    "e.printFeatureEvalPlot()\n",
    "e.printFeatureEvalMatrix()\n",
    "\n",
    "#print out the shapes of result data\n",
    "#print(np.array(e.getHighestScores()).shape)\n",
    "print(np.array(e.getResults()).shape)\n",
    "#print(np.array(e.getMeanScores()).shape)\n",
    "#print(e.getHighestMeanScores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
